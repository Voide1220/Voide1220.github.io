

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->
 <script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

</style>

	<title>Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training</span><br><br><br>
	</center>
	
	<table align="center" width="800px">
            <tbody><tr>
              <td align="center" width="160px">
                <center>
                  <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=x0Uk7S8AAAAJ&hl=zh-CN&oi=ao">Haicheng Wang</a><sup>1,2*</sup></span>
                  </center>
                  </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://voide1220.github.io/">Chen Ju</a><sup>1*✉️</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://weixionglin.github.io/me/">Weixiong Lin</a><sup>2</sup></span>
                </center>
			    
              </td>
              <td align="center" width="160px">	    
        <center>
          <span style="font-size:16px"><a href="https://sites.google.com/view/xiao-shuai/home">Shuai Xiao</a><sup>1✉️</sup></span>
          </center>  			    
       </tr>
        </tbody></table><br>
			    
			    
		
	<table align="center" width="800px">	    
                <tr>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=LcoK9ZEAAAAJ&hl=zh-CN&oi=ao">Mengting Chen</a><sup>1</sup></span>
                </center> 				
			
		</td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=jCKSKmYAAAAJ&hl=zh-CN&oi=ao">Yixuan Huang</a><sup>1</sup></span>
                </center> 
			    
		</td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=4Cx0DyEAAAAJ&hl=zh-CN&oi=sra">Chang Liu</a><sup>2</sup></span>
                </center>      
			    
                </td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="">Mingshuai Yao</a><sup>1</sup></span>
                </center>  			    
            </tr>
		
        </tbody></table><br>
	
  <table align="center" width="800px">	    
          <tr>
              <td align="center" width="160px">	    
        <center>
          <span style="font-size:16px"><a href="">Jinsong Lan</a><sup>1</sup></span>
          </center> 				

</td>
              <td align="center" width="160px">	    
        <center>
          <span style="font-size:16px"><a href="https://scholar.google.com/citations?user=NpTmcKEAAAAJ&hl=en">Ying Chen</a><sup>1</sup></span>
          </center> 
    
</td>
              <td align="center" width="160px">	    
        <center>
          <span style="font-size:16px"><a href="">Qingwen Liu</a><sup>1</sup></span>
          </center>      
    
          </td>
              <td align="center" width="160px">	    
        <center>
          <span style="font-size:16px"><a href="https://scholar.google.com/citations?hl=en&user=x_sgJskAAAAJ">Yanfeng Wang</a><sup>2</sup></span>
          </center>  			    
      </tr>

  </tbody></table><br>	
	
	  <table align="center" width="700px">
         <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Alibaba Group</span>
                </center>

			    
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai Jiao Tong University</span>
                </center>
                </td>
        </tr></tbody>
	</table>
	
			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=500px>
						<center>
							<span style="font-size:22px">
								<span style="font-size:20px">CVPR 2025</span>
							</span>
						</center>
					</td>
				</tr>
			</table>	
	

      <br><hr>
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://github.com/anakin-skywalker-Joseph/Holistic-CLIP"> Code </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://arxiv.org/abs/2412.00440"> Paper </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="./cite_holism.txt"> Bibtex </a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>


      <br><hr>
	<center><h1>Problem</h1></center>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/intro.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
	      </center>
		  
	      </div>
        </center>
      </center>
	<p> <strong>Myopia</strong>. OpenAI's CLIP uses crude (image, text) web data for one-to-one contrastive alignment, causing serious myopia, i.e., bias to monotonous short texts and shallow visual expressivity. <strong>Holism</strong>. We advance one holistic CLIP paradigm, by updating colorful (image, multi-texts) data from diverse views, levels; and designing multi-to-multi constrastive learning for image-text part-to-part matching.  </p>

      <br><hr>
        <table align=center width=850px>
	<center><h1>Abstract</h1></center>
	<tr>
		<td>
      In rapidly evolving field of vision-language models (VLMs), contrastive language-image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive paradigm to learn alignment from large-scale messy web data, CLIP faces a serious myopic dilemma, resulting in biases towards monotonous short texts and shallow visual expressivity. To overcome these issues, this paper advances CLIP into one novel holistic paradigm, by updating both diverse data and alignment optimization. To obtain colorful data with low cost, we use image-to-text captioning to generate multi-texts for each image, from multiple perspectives, granularities, and hierarchies. Two gadgets are proposed to encourage textual diversity. To match such (image, multi-texts) pairs, we modify the CLIP image encoder into multi-branch, and propose multi-to-multi contrastive optimization for image-text part-to-part matching. As a result, diverse visual embeddings are learned for each image, bringing good interpretability and generalization. Extensive experiments and ablations across over ten benchmarks indicate that our holistic CLIP significantly outperforms existing myopic CLIP, including image-text retrieval, open-vocabulary classification, and dense visual tasks. Code for holistic CLIP will be released upon publication, to further promote the prosperity of VLMs.   
		</td>
	</tr>
	</table>
     
	
      <br><hr>
	<center><h1>Framework Overview</h1></center>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/pipeline.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
	      </center>
	      </div>
        </center>
      </center>
	<p> <strong>Pipeline Overview of Holistic CLIP</strong>. To evolve data from monotonous (image, text) pairs to colorful (image, multi-texts) pairs, we use powerful VLMs for captioning from multiple views, levels, and granularities. Diverse prompts are defined to encourage diversity. We then modify the CLIP image encoder into multi-branch, and optimize by multi-to-multi constrastive learning for part-to-part matching. During inference, flexible embedding customizations are available for different tasks, showing good interpretability and generalization.
        <br><hr>

        <center><h1>Model Structure</h1></center>
        <br>
          <center>
            <div class="container">
          <div class="row">
            <center>
              <img src="./image/frame.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
            </center>
            
            </div>
          </center>
        </center>
      <p> <strong>Architecture Overview of Holistic CLIP</strong>. To generate \( H \) image features, we leverage two different structures: \( \Psi_{\mathrm{CLS}} \) and \( \Psi_{\mathrm{MLP}} \). Then we match \( H \) image features with \( M \) text features. Normally \( H=M\) and we apply one-to-one matching. 
        <br><hr>

	<center><h1>Experiments</h1></center>
  <center><h2>Elementary Short-Text Retrieval</h2></center>
        <p><center><img class="center"  src="./image/exp1.PNG" width="900px"></center></p>
  <center><h2>Complex Long-Text Retrieval</h2></center>
        <p><center><img class="center"  src="./image/exp2.PNG" width="900px"></center></p>
  <center><h2>Zero-Shot Image Classification</h2></center>
        <p><center><img class="center"  src="./image/exp3.PNG" width="900px"></center></p>
  <center><h2>Image-to-Text Captioning & All-Round Abilities</h2></center>
        <p><center><img class="center"  src="./image/exp4.PNG" width="600px"></center></p>
        <p><center><img class="center"  src="./image/exp5.PNG" width="900px"></center></p>

	<center><h1>Ablation Study</h1></center>
	<center><h2>Ablation Study for the Embedding Fusion</h2></center>
        <p><center><img class="center"  src="./image/exp6.PNG" width="600px"></center></p>

	<center><h2>Embedding Customization</h2></center>
        <p><img class="center"  src="./image/exp7.PNG" width="800px"></p>

	<center><h2>Ablation Study for the Text Number on CC3M</h2></center>
        <p><center><img class="center"  src="./image/exp8.PNG" width="600px"></center></p>
        
    <center><h1>Visualization</h1></center>
        <p><center><img class="center"  src="./image/vis.PNG" width="600px"></center></p>
    <p> <strong>Attention Visualization of Holistic CLIP's Vision</strong>. Vision is naturally decomposed by aligning with various texts. 
      <br>
      <hr>
      <center> <h1> Acknowledgements </h1> </center>
	<p>
		This research is supported by Alibaba Group.
	</p>
  
      <br>


<br>
</body>
</html>
