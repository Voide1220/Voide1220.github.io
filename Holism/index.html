

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training</span><br><br><br>
	</center>
	
	<table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=x0Uk7S8AAAAJ&hl=zh-CN&oi=ao">Haicheng Wang*</a><sup>1,2</sup></span>
                </center>
			    
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://voide1220.github.io/">Chen Ju*</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/members/">Haozhe Cheng</a><sup>2</sup></span>
                </center>    	    
                </tr>
        </tbody></table><br>

	<table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=6Qa2JCwAAAAJ&hl=zh-CN">Xu Chen</a><sup>1</sup></span>
                </center>
			    
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=o4SDCAYAAAAJ&hl=zh-CN&oi=ao">Zhonghua Zhai</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://whuang.org/">Weilin Huang</a><sup>1</sup></span>
                </center>    	    
                </tr>
        </tbody></table><br>
	
		
	<table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="">Jinsong Lan</a><sup>1</sup></span>
                </center>
			    
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://sites.google.com/view/xiao-shuai/home">Shuai Xiao</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=3gHhO9QAAAAJ&hl=zh-CN&oi=ao">Bo Zheng</a><sup>1</sup></span>
                </center>    	    
                </tr>
        </tbody></table><br>

	
	  <table align="center" width="700px">
         <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Alibaba Group</span>
                </center>

			    
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai Jiao Tong University</span>
                </center>
                </td>
        </tr></tbody>
	</table>
	
			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=500px>
						<center>
							<span style="font-size:22px">
								<span style="font-size:20px">ArXiv</span>
							</span>
						</center>
					</td>
				</tr>
			</table>	
	

      <br><hr>
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://github.com/anakin-skywalker-Joseph/Holistic-CLIP"> Code </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://arxiv.org/pdf/2407.11717"> Paper </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="./cite_holism.txt"> Bibtex </a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>


      <br><hr>
	<center><h1>Problem</h1></center>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/intro.png" alt="alt text" style="width: 80%; object-fit: cover; max-width:80%;"></a>
	      </center>
		  
	      </div>
        </center>
      </center>
	<p> Myopia: OpenAI's CLIP uses crude (image, text) web data for one-to-one contrastive alignment, causing serious myopia, i.e., bias to monotonous short texts and shallow visual expressivity. Holism: We advance one holistic CLIP paradigm, by updating colorful (image, multi-texts) data from diverse views, levels; and designing multi-to-multi constrastive learning for image-text part-to-part matching. </p>

      <br><hr>
        <table align=center width=850px>
	<center><h1>Abstract</h1></center>
	<tr>
		<td>
            In rapidly evolving field of vision-language models (VLMs), contrastive language-image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive paradigm to learn alignment from large-scale messy web data, CLIP faces a serious myopic dilemma, resulting in biases towards monotonous short texts and shallow visual expressivity. To overcome these issues, this paper advances CLIP into one novel holistic paradigm, by updating both diverse data and alignment optimization. To obtain colorful data with low cost, we use image-to-text captioning to generate multi-texts for each image, from multiple perspectives, granularities, and hierarchies. Two gadgets are proposed to encourage textual diversity. To match such (image, multi-texts) pairs, we modify the CLIP image encoder into multi-branch, and propose multi-to-multi contrastive optimization for image-text part-to-part matching. As a result, diverse visual embeddings are learned for each image, bringing good interpretability and generalization. Extensive experiments and ablations across over ten benchmarks indicate that our holistic CLIP significantly outperforms existing myopic CLIP, including image-text retrieval, open-vocabulary classification, and dense visual tasks.		
    </td>
	</tr>
	</table>
     
	
      <br><hr>
	<center><h1>Framework Overview</h1></center>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/frame.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
	      </center>
	      
	      </div>
        </center>
      </center>
	<p> Pipeline Overview of Holistic CLIP. To evolve data from monotonous (image, text) pairs to colorful (image, multi-texts) pairs, we use powerful VLMs for captioning from multiple views, levels, and granularities. Diverse prompts are defined to encourage diversity. We then modify the CLIP image encoder into multi-branch, and optimize by multi-to-multi constrastive learning for part-to-part matching. During inference, flexible embedding customizations are available for different tasks, showing good interpretability and generalization.
        <br><hr>

        <center><h1>Empirical Study</h1></center>
        <br>
          <center>
            <div class="container">
          <div class="row">
            <center>
              <img src="./image/diag.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
            </center>
            
            </div>
          </center>
        </center>
      <p> Empirical Evaluation of Token Redundancy & Attention Concentration on BLIP fine-tuned for multi-modal retrieval. Results reveal the non-negligible redundancy in the token sequence from perspectives of semantics and similarity.
        <br><hr>

	<center><h1>Experiments</h1></center>
        <p><img class="center"  src="./image/expb.png" width="900px"></p>
        <p><img class="center"  src="./image/expa.png" width="900px"></p>


	<center><h1>Ablation Study</h1></center>
        <p><center><img class="center"  src="./image/ablation.png" width="400px"></center></p>
	<p> Ablation Study on Drop Ratio. Semantic value retains superior performance when ratio is small, mutual redundancy possesses better stability on the large ratio. By combining two components, Turbo gets competitive results and stability on the whole scope.
        <p><center><img class="center"  src="./image/exp5.png" width="600px"></center></p>

	<center><h1>Robustness</h1></center>
        <p><center><img class="center"  src="./image/robust.png" width="400px"></center></p>
	<p> Ablation Study of Balancing Coefficient. On image captioning using BLIP (VIT-Base and VIT Large), these results prove our robustness, as the performance varies slightly.

	<center><h1>Visualization Results</h1></center>
        <p><center><img class="center"  src="./image/merge.png" width="600px"></center></p>
	<p> Left: Turbo merges background patches, while retains foreground patches with semantics, preserving more key information. Right: The quality
        of text-to-image generation is close before and after Turbo acceleration.
        
    <center><h1>Comparison with Previous SOTA</h1></center>
        <p><center><img class="center"  src="./image/res.png" width="600px"></center></p>
    <p> Generation comparisons for ToMe and Turbo. Compared with ToMe, Turbo retains more details and has better quality.  
      <br>
      <hr>
      <center> <h1> Acknowledgements </h1> </center>
	<p>
		This research is completed during research internship, supported by Alibaba Group. 
	</p>
  
      <br>


<br>
</body>
</html>

