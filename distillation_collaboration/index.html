
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization</span><br><br><br>
	</center>
	<table align="center" width="1400px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://ju-chen.github.io/">Chen Ju</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://dyekuu.github.io/">Kunhao Zheng</a><sup>1</sup></span>
                </center>
                
	    	</td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=wSRKaWIAAAAJ&hl=zh-CN&oi=ao">Jinxiang Liu</a><sup>1</sup></span>
                </center>
		    
		    </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=hCr8Km8AAAAJ&hl=zh-CN&oi=ao">Peisen Zhao</a><sup>2</sup></span>
                </center>
			    
                </td>
                    <td align="center" width="160px">
	            <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1</sup></span>
                </center>

                </td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=RDwnNsQAAAAJ&hl=zh-CN">Jianlong Chang</a><sup>2</sup></span>
                </center> 			    

		<br/>
			    
                </td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com.hk/citations?user=61b6eYkAAAAJ&hl=zh-CN">Qi Tian</a><sup>2</sup></span>
                </center> 
			    
		</td>
                    <td align="center" width="160px">	    
              <center>
                <span style="font-size:16px">Yanfeng Wang<sup>1</sup></span>
                </center>      
				    
            </tr>

        </tbody></table><br>
	
	  <table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
			    
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Huawei Cloud & AI</span>
                </center>
                </td>
        </tr></tbody></table>
	
			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=500px>
						<center>
							<span style="font-size:22px">
								<span style="font-size:20px">CVPR 2023</span>
							</span>
						</center>
					</td>
				</tr>
			</table>	
	

      <br><hr>
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://github.com/ju-chen/Efficient-Prompt"> Code </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="https://arxiv.org/pdf/2212.09335.pdf"> Paper </a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    <a href="./cite_distilling.txt"> Bibtex </a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>


      <br><hr>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/intro.PNG" alt="alt text" style="width: 80%; object-fit: cover; max-width:80%;"></a>
	      </center>
	      </div>
        </center>
      </center>
	
	

      <br><hr>
        <table align=center width=850px>
	<center><h1>Abstract</h1></center>
	<tr>
		<td>
			Weakly-supervised temporal action localization (WTAL) learns to detect and classify action instances with only category labels. Most methods widely adopt the off-the-shelf Classification-Based Pre-training (CBP) to generate video features for action localization. However, the different optimization objectives between classification and localization, make temporally localized results suffer from the serious incomplete issue. To tackle this issue without additional annotations, this paper considers to distill free action knowledge from Vision-Language Pre-training (VLP), as we surprisingly observe that the localization results of vanilla VLP have an over-complete issue, which is just complementary to the CBP results. To fuse such complementarity, we propose a novel distillation-collaboration framework with two branches acting as CBP and VLP respectively. The framework is optimized through a dual-branch alternate training strategy. Specifically, during the B step, we distill confident background pseudo-labels from the CBP branch; while during the F step, confident foreground pseudo-labels are distilled from the VLP branch. As a result, the dual-branch complementarity is effectively fused to promote one strong alliance. Extensive experiments and ablations on THUMOS14 and ActivityNet1.2 reveal that our method significantly outperforms state-of-the-art methods.
		</td>
	</tr>
	</table>
     
	<center><h1>Distillation-Collaboration Framework</h1></center>

	
      <br><hr>
      <br>
        <center>
          <div class="container">
	    <div class="row">
	      <center>
	        <img src="./image/frame.PNG" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
	      </center>
	      </div>
        </center>
      </center>
	<p><b> It covers two parallel branches, named CBP and VLP, and is optimized by an alternating strategy. We warm up the CBP branch in advance. In B step, we freeze both CLIP encoders, and distill confident background pseudo-labels from the CBP branch, to train prompt vectors and temporal Transformer in the VLP branch. In F step, confident foreground pseudo-labels are distilled for the CBP branch. We utilize both knowledge distillation loss and contrastive loss during dual-branch collaboration. </b> </p>
	

       <center><h1>Results</h1></center>

       <p><b> Ablation Studies </b> </p>
       <p><img class="center"  src="./image/ablations.png" width="800px"></p>


       <p><b> Comparisons with SOTA on THUMOS14 </b> </p>
       <p><img class="center"  src="./image/thumos.PNG" width="800px"></p>
	

	<p><b> Qualitative Detection Results </b> </p>
       <p><img class="center"  src="./image/result.png" width="800px"></p>
	


      <br>
      <hr>
      <center> <h1> Acknowledgements </h1> </center>
	<p>
		This research is supported by the National Key R&D Program of China (No. 2022ZD0160702), STCSM (No. 22511106101, No. 18DZ2270700, No. 21DZ1100100), 111 plan (No. BP0719010), and State Key Laboratory of UHD Video and Audio Production and Presentation. Weidi Xie would like to acknowledge the generous support of Yimeng Long and Yike Xie.
	</p>
  
      <br>


<br>
</body>
</html>
